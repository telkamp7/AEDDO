\chapter{Methods}

In this chapter, the current state-of-the-art methods for disease outbreak detection will be outlined. Furthermore, the novel outbreak detection algorithm will be introduced, along with the theory related to generalized mixed effects models and hierarchical generalized linear models. These modeling frameworks are utilized in this master thesis to analyze the count observations denoted as $\boldsymbol{y}$, but more importantly they play a crucial role in assessing the unobserved random variables or random effects represented by $\boldsymbol{u}$, which are directly employed in the detection algorithm for characterizing outbreaks.

Due to the complexity of generalized mixed effects models, obtaining closed-form solutions is generally not feasible. Therefore, an overview of the Laplace approximation technique will be provided in this chapter, which allows for approximating the likelihood function in these models. Additionally, the implementation of these models in the R programming language will be presented.

\section{State-of-the-art outbreak detection algorithm}\label{StateOfTheArt}

In this section, the original method introduced by @Farrington_1996 and the subsequent improvements proposed by @Noufaily_2013 will be outlined. These methods are recognized as the current state-of-the-art for disease outbreak detection and will be used as benchmarks to evaluate the performance of the novel outbreak detection algorithm proposed in this thesis. Both of the aforementioned methods have been implemented in the R package called \textbf{surveillance} by @Salmon_2016, which can be accessed from the \gls{CRAN} at \href{https://cran.r-project.org/web/packages/surveillance/index.html}{\nolinkurl{https://cran.r-project.org/web/packages/surveillance/index.html}}. The presentation of these methods are strongly inspired by @Salmon_2016.

Both methods follow the same steps in the algorithm. The first step involves fitting an over-dispersed Poisson \gls{GLM} with a log link to the reference data. In this model, the baseline count $y_t$ corresponding to the baseline time point $t$ is assumed to have an expected value $\lambda_t$ and a variance $\phi\lambda_t$, where $\phi\geq1$ is ensured to account for over-dispersion. The systematic component of the model includes only a linear time trend in the frequency of reports. Therefore, the systematic component can be expressed as

\begin{equation}
  \log(\lambda_t)=\alpha+\beta t
\end{equation}

The original method incorporated seasonal effects by considering counts from comparable periods in past years for threshold calculation. This approach is similar to the one used by @Stroup_1989. The baseline weeks, which are used as reference, are determined by two integers: $b$ the number of years back, and $w$ the window half-width. For a given current week $x$ of year $h$, only data from weeks $x-w$ to $x+w$ of years $h-b$ to $h-1$ are considered, resulting in a total of $n=b(2w+1)$ baseline weeks. The default values are $b=5$ and $w=3$, resulting in a total of $n=35$ baseline values. 

However, @Noufaily_2013 demonstrated that the algorithm performs better when utilizing more historical data without disregarding seasonality. To achieve this, the author introduced a 10-level factor with a 7-week reference period and nine additional 5-week periods in each year. As a result, the systematic component of the model is modified as follows

\begin{equation}
\log(\lambda_t) = \alpha + \beta t + \delta_{j(t)}
\end{equation}

In this equation, $j(t)$ represents the seasonal factor level corresponding to time point $t$. The reference week $t_0$ is always associated with the reference seasonal level, denoted by $j(t_0) = 0$ and $\delta_0 = 0$.

The idea of incorporating more data while preserving seasonality has been further expanded in the implementation of the method in the \textbf{surveillance} R package. The package allows the user to choose an arbitrary number of periods in each year. Consequently, the systematic component is adjusted as follows

\begin{equation}
\log(\lambda_t) = \alpha + \beta t + \delta_{c(t)}
\end{equation}

In this equation, $c(t)$ represents the coefficients of a zero-order spline with $\mathtt{noPeriods}+1$ knots. It can be conveniently represented as a `noPeriods`-level factor that captures seasonality. The function $c(t)$ indicates which season or period of the year $t$ belongs to.

Furthermore, @Noufaily_2013 demonstrated that it is beneficial to exclude the last 26 weeks before $t_0$ from the baseline calculation. This exclusion helps prevent a reduction in sensitivity when an outbreak has recently started before $t_0$.

In the second step, the algorithm predicts the expected number of counts $\lambda_{t_0}$ for the current time point $t_0$ using the fitted \gls{GLM}. Both methods differ in their assumptions for calculating the upper bound $U_{t_0}$.

The original method assumes that a transformation of the prediction error, denoted as $g(y_{t_0}-\hat{\lambda}_{t_0})$, follows a normal distribution. For example, when using the identity transformation $g(x)=x$, the assumption becomes

\begin{equation}
y_{t_0} - \hat{\lambda}_{t_0} \sim \N \big(0, \V(y_{t_0}-\hat{\lambda}_{t_0})\big)
\end{equation}

The upper bound of the prediction interval is then calculated based on this distribution. The variance of the prediction error is given by

\begin{equation}
\V(y_{t_0}-\hat{\lambda}_{t_0}) = \V(y_{t_0}) + \V(\hat{\lambda}_{t_0}) = \phi\lambda_{t_0} + \V(\hat{\lambda}_{t_0})
\end{equation}

Here, $\V(\hat{y}_{t_0})$ represents the variance of an observation, and $V(\hat{\lambda}_{t_0})$ represents the variance of the estimate. The threshold, defined as the upper bound of a one-sided $(1-\alpha)\cdot100\%$ prediction interval, is calculated as

\begin{equation}
U_{t_0} = \hat{\lambda}_{0} + z_{1-\alpha}\hat{\V}\bigg(\sqrt{y_{t_0}-\hat{\lambda}_{t_0}}\bigg)
\end{equation}

However, this method's weakness lies in the assumption of normality itself. Therefore, an alternative assumption was presented in @Noufaily_2013. This approach assumes that $y_{t_0}$ follows a negative binomial distribution, denoted as $\text{NB}(\lambda_{t_0}, \nu)$, where $\lambda_{t_0}$ represents the mean and $\nu = \frac{\lambda_{t_0}}{\phi-1}$ represents the over-dispersion parameter. In this parameterization, the expected value of $y_t$ remains $\lambda_t$, and the variance of $y_t$ is $\phi\lambda_t$, with $\phi > 1$. If $\phi \leq 1$, a Poisson distribution is assumed for the observed count. The threshold is defined as a quantile of the negative binomial distribution using the plug-in estimates $\hat{\lambda}_{t_0}$ and $\hat{\phi}$.

In the final step, the observed count $y_{t_0}$ is compared to the upper bound $U_{t_0}$, and an alarm is raised if $y_{t_0} > U_{t_0}$. The fitting of the \gls{GLM} in both methods involves three important steps.

First, the algorithm optionally performs a power transformation to correct for skewness and stabilize the variance of the data.

Next, the significance of the time trend is checked. The time trend is included in the model only if it is statistically significant at a chosen significance level, there are more than three years of reference data, and there is no over-extrapolation due to the time trend.

Finally, past outbreaks are reweighted based on their Anscombe residuals. If the Anscombe residual of a count exceeds a certain weight threshold, it is re-weighted in a second fitting of the \gls{GLM}. In the original method by @Farrington_1996, a re-weighting threshold of 1 was used. However, @Noufaily_2013 suggests using a value of 2.56 for the weight threshold to make the re-weighting procedure less drastic, as it also reduces the variance of the observations. For a more detailed description of the re-weighting scheme employed in the Farrington and Noufaily method, refer to Section 3.6 in @Farrington_1996 and Section 2.1 in @Noufaily_2013, respectively.

\section{Novel outbreak detection algorithm}\label{Novel}

In this section, the novel algorithm for the prospective detection of disease outbreaks proposed in this thesis is outlined. The algorithm utilizes a generalized mixed effects model or a hierarchical generalized linear model as a modeling framework to model the count observations $\boldsymbol y$ and assess the unobserved random effects $\boldsymbol u$. These random effects are used directly in the detection algorithm to characterize an outbreak. The theoretical foundations of these models will be further discussed in Sections \ref{GLMM} and \ref{HLMM}.

The first step involves fitting either a hierarchical Poisson Normal or Poisson Gamma model with a log link to the reference data. Here, it is possible for the user to include an arbitrary number of covariates by supplying a model formula. In order to account for structural changes in the time series, e.g. an improved and more sensitive diagnostic method or a new screening strategy at hospitals, a rolling window with width $k$ is used to estimate the time-varying model parameters. Also, it is assumed that the count is proportional to the population size $\boldsymbol n$. Hence, in terms of the canonical link the model for the fixed effects is

\begin{equation}
  \log(\lambda_{it})=\boldsymbol x_{it} \boldsymbol\beta + \log(n_{it}), \quad i=1,\dots,m, \quad t=1,\dots,T
\end{equation}

Here $\boldsymbol x_{it}$ and $\boldsymbol\beta$ are $p$-dimensional vectors of covariates and fixed effects parameters respectively, where $p$ denotes the number of covariates or fixed effects parameters, $m$ denotes the number of groups, and $T$ denotes the length of the period.

In the second step of the algorithm, as a new observation becomes available, the algorithm infers the one-step ahead random effect $u_{i{t_1}}$ for each group using the obtained model estimates $\hat{\theta}_{t_0}$. Here, $t_0$ represents the current time point, and $t_1$ represents the one-step ahead time point. The threshold $U_{t_0}$ for detecting outbreaks is defined as a quantile of the distribution of the random effects in the second stage model. This threshold can be calculated based on either a Gaussian distribution using the plug-in estimate $\hat{\sigma}_{t_0}$ or a Gamma distribution using the plug-in estimate $\hat{\phi}_{t_0}$. The choice of distribution depends on the specific modeling framework and assumptions used in the analysis.

In the final step, the inferred random effect $\hat{u}_{i{t}_1}$ is compared to the upper bound $U_{t_0}$, and an alarm is raised if $\hat{u}_{i{t}_1}>U_{t_0}$. If an outbreak is detected, the related observation $y_{it_1}$ is omitted from the parameter estimation in the future. Thus, resulting in a smaller sample size for the rolling window until that specific observation is discarded.

The above algorithm is implemented in an R function, which can be accessed from the associated GitHub repository at \href{https://github.com/telkamp7/AEDDO/blob/main/src/models/aeddo.R}{src/models/aeddo.R}.

\section{General mixed effects models}\label{GLMM}

In this section selected theory related to generalized mixed effects models is presented. The presentation of Section \@ref(GLMM) and Section \@ref(HLMM) is mostly inspired by @Madsen_2010. 

The general mixed effects model can be represented by its likelihood function

\begin{equation}\label{eq:glmm}
  L_{M}(\boldsymbol{\theta; y})=\int_{\mathbb{R}^{q}} L(\boldsymbol{\theta;u,y}) d\boldsymbol{u}
\end{equation}

where $\boldsymbol{y}$ is the observed random variable, $\boldsymbol{\theta}$ is the model parameters to be estimated and $\boldsymbol{U}$ is the $q$ unobserved random variables. The likelihood function $L$ is the joint likelihood of both the observed and the unobserved random variables. The likelihood function for estimating $\boldsymbol{\theta}$ is the marginal likelihood $L_{M}$ obtained by integrating out the unobserved random variables. In general it is difficult to solve the integral in \eqref{eq:glmm} if the number of unobserved random variables is more than a few and hence numerical methods must be used. Thus, an outline of the Laplace approximation is included in this section.

\subsection{Hierarchical models}\label{hierarchicalModels}

It is useful to formulate the model as a hierarchical model containing a \textit{first stage model}

\begin{equation}\label{eq:firstStage}
  f_{Y|u}(\boldsymbol{y;u,\beta})
\end{equation}

which is a model for the observed random variables given the unobserved random variables, and a \textit{second stage model}

\begin{equation}\label{eq:secondStage}
  f_{U}(\boldsymbol{u; \Psi})
\end{equation}

which is a model for the unobserved random variables. Here $\boldsymbol{\beta}$ represent the fixed effects parameters and $\boldsymbol{\Psi}$ is a model parameter. The total set of parameters is $\boldsymbol{\theta}=(\beta, \Psi)$. Hence the joint likelihood is given as

\begin{equation}\label{eq:jl}
  L(\boldsymbol{\beta, \Psi; u, y})=f_{Y|u}(\boldsymbol{y;u,\beta}) f_{U}(\boldsymbol{u; \Psi})
\end{equation}

To obtain the likelihood for the model parameters $(\boldsymbol{\beta, \Psi})$ the unobserved random variables are integrated out. The likelihood function for estimating $(\boldsymbol{\beta, \Psi})$ is as in \eqref{eq:glmm} the marginal likelihood

\begin{equation}\label{eq:glmm2}
  L_{M}(\boldsymbol{\beta, \Psi; y})=\int_{\mathbb{R}^{q}} L(\boldsymbol{\beta, \Psi;u,y}) d\boldsymbol{u}
\end{equation}

where $q$ is the number of unobserved random variables, and $\boldsymbol{\beta}$ and $\boldsymbol{\Psi}$ are the parameters to be estimated.

\subsection{Laplace Approximation}

The Laplace approximation will be outlined in the following. A thorough description of the Laplace approximation in nonlinear mixed effects models is found in @Wolfinger_1997.

For a given set of model parameters $\boldsymbol{\theta}$ the joint log-likelihood $\ell(\boldsymbol{\theta, u, y})=\log\big(L(\boldsymbol{\theta, u, y})\big)$ is approximated using a second order Taylor approximation around the optimum $\boldsymbol{\tilde{u}}=\boldsymbol{\hat{u}_\theta}$ of the log-likelihood function w.r.t. the unobserved random variables $\boldsymbol{u}$, i.e.,

\begin{equation}\label{eq:laplaceApprox}
  \ell(\boldsymbol{\theta, u, y})\approx\ell(\boldsymbol{\theta, \tilde{u}, y}) - \frac{1}{2}(\boldsymbol{u-\tilde{u}})^T \boldsymbol{H}(\boldsymbol{\tilde{u}})(\boldsymbol{u-\tilde{u}})
\end{equation}

where the first-order term of the Taylor expansion disappears since the expansion is done around the optimum $\boldsymbol{\tilde {u}}$ and $\boldsymbol{H}(\boldsymbol{\tilde{u}})=-\ell_{uu}''(\boldsymbol{\theta, u, y})|_{\boldsymbol{u=\tilde{u}}}$ is the negative Hessian of the joint log-likelihood evaluated at $\boldsymbol{\tilde{u}}$. 

It is readily seen that the joint log-likelihood for the hierarchical model specified in \eqref{eq:firstStage} and \eqref{eq:secondStage} is

\begin{equation}
  \ell(\boldsymbol{\theta, u, y}) = \ell(\boldsymbol{\beta, \Psi, u, y}) = \log f_{Y|u}(\boldsymbol{y;u,\beta})+\log f_U(\boldsymbol{u;\Psi})
\end{equation}

which implies that the Laplace approximation becomes

\begin{equation}
  \ell_{M,LA}(\boldsymbol{\theta, y})=\log f_{Y|u}(\boldsymbol{y; \tilde{u},\beta})+\log f_U(\boldsymbol{\tilde{u}, \Psi})-\frac{1}{2}\log\Bigg|\frac{\boldsymbol{H}(\boldsymbol{\tilde{u}})}{2\pi}\Bigg|
\end{equation}

\subsection{Formulation of the generalized mixed effects model}

The generalized mixed effects model utilized in this thesis to model the count observations $\boldsymbol y$ and assess the random effects $\boldsymbol u$ is presented in Definition \@ref(def:poisnTheorem), along with the joint likelihood function for the first and second stage models.

::: {.definition #poisnTheorem name="Hierarchical Poisson Normal model"}
In order to simplify the notation, the probability density functions are presented for a specific observation. Hence, the subscripts indicating the group and time are omitted. The conditional distribution of the count observations is assumed to be a Poisson distribution with intensities $\boldsymbol \lambda$

\begin{equation}
  f_{Y|u}(y; u, \boldsymbol{\beta})=\frac{\lambda\exp(u)^{y}}{y!}\exp\big(-\lambda\exp(u)\big)
\end{equation}

Also, it is assumed that the count is proportional to the population size $\boldsymbol{n}$. Hence, in terms of the canonical link for the Poisson distribution the model for the fixed effects is

\begin{equation}
\log(\lambda_{it})=\boldsymbol x_{it} \boldsymbol\beta + \log(n_{it}), \quad i=1,\dots,m, \quad t=1,\dots,T
\end{equation}

The probability density function for the distribution of the random effects is assumed to follow a zero mean Gaussian distribution, $\boldsymbol u\sim\N(\boldsymbol 0,I\sigma^2)$, i.e.

\begin{equation}
  f_U(u;\sigma)=\frac{1}{\sigma\sqrt{2\pi}}\exp\Bigg(-\frac{u^2}{2\sigma^2}\Bigg)
\end{equation}

where $\sigma$ is a model parameter. 

Henceforth, the total set of parameters are $\boldsymbol{\theta}=(\boldsymbol{\beta},\sigma)$ and the model can be formulated as a two-level hierarchical model

\begin{subequations} \label{eq:PoisN}
  \begin{alignat}{2}
    \boldsymbol{Y|u} &\sim \Pois \big( \boldsymbol{\lambda} \exp(\boldsymbol{u}) \big) \label{eq:pois_n0} \\ 
    \boldsymbol{u} &\sim \N(\boldsymbol{0},I\sigma^2) \label{eq:pois_n1}
  \end{alignat}
\end{subequations}

The joint likelihood for the count observations $\boldsymbol y$ and the random effects $\boldsymbol u$ becomes

\begin{multline}\label{eq:jnllPoisN}
  L(\boldsymbol{\beta}, \sigma;u_{it},y_{it})=\\
  \prod_{t=1}^{T}\prod_{i=1}^{m} \frac{\big(\lambda_{it}\exp(u_{it})\big)^{y_{it}}}{y_{it}!}\exp\big(-\lambda_{it}\exp(u_{it})\big) \prod_{t=1}^{T}\prod_{i=1}^{m} \frac{1}{\sigma\sqrt{2\pi}}\exp\Bigg(-\frac{u_{it}^2}{2\sigma^2}\Bigg)
\end{multline}
:::

\section{Hierarchical generalized linear models}\label{HLMM}

In this section selected theory related to hierarchical generalized linear models is presented. The model class was initially formulated by @Lee_1996 as a natural generalization of the generalized linear models to also incorporate random effects. A starting point in hierarchical modelling is an assumption that the distribution of random effects may be modeled by an exponential dispersion family. This family of models were first introduced by @Fisher_1922, and has proven to play an important role in mathematical statistics because of their simple inferential properties. The exponential dispersion family considers a family of distributions, which can be written on the form

\begin{equation}\label{eq:expDispFam}
  f_Y(y;\theta)=c(y,\phi)\exp\big(\phi \{\theta y-\kappa(\theta) \}\big)
\end{equation}

Here the parameter $\phi>0$ is called the \textit{precision parameter}, which in some cases represents a shape parameter as for the Gamma distribution. In other cases the precision parameter represents an over-dispersion that is not related to the mean. These distributions combine with the so-called \textit{standard conjugate distributions} in a simple way, and lead to marginal distributions that may be expressed in a closed form suited for likelihood calculations. For an introduction to the concept of \textit{standard conjugate distributions} and the definition of a hierarchical generalized linear model, refer to Section 6.3 and Section 6.5 of @Madsen_2010, respectively.

In general, when the conditional distribution of $Y|u$ is a Poisson distribution, and the conjugate distribution is assumed to be a Gamma distribution with mean value 1, it follows that the distribution of $Y$ is a negative binomial distribution.

To further motivate this choice of distribution for the second stage model, an illustrative example focusing on \glsfirst{LIST} is presented.

::: {.example #poisgExample name="Variation between observed cases of LIST"}

```{r PoisGexample, echo=FALSE}

dat <- read_rds("../data/processed/dat5.rds")

LIST <- dat %>%
  filter(caseDef == "LIST") %>% 
  group_by(Date) %>%
  reframe(y = sum(cases), n = sum(n))

momentsLIST <- LIST %>%
  summarize(var(y), mean(y))

poisNBExp <- tibble(y = as_factor(0:20), 
                  poisExp = dpois(x = as.integer(y), lambda = momentsLIST$`mean(y)`) * nrow(LIST),
                  NBExp = dnbinom(x = as.integer(y), prob = momentsLIST$`mean(y)`/momentsLIST$`var(y)`, size = momentsLIST$`mean(y)`^2/(momentsLIST$`var(y)`-momentsLIST$`mean(y)`)) * nrow(LIST)) %>%
  mutate(y = fct_collapse(y, `10+`=c(as.character(10:20)))) %>%
  group_by(y) %>%
  reframe(sum(poisExp), sum(NBExp))

meanYExample <- round(momentsLIST$`mean(y)`,2)
varYExample <- round(momentsLIST$`var(y)`,2)

```

Usually, it is reasonable to assume a Poisson distribution for count data, where the expected value and variance are both equal to $\lambda$. However, it should be noted that this assumption may not always hold true when modeling count data.

```{r PoisGexampleTbl, echo=FALSE}

LIST %>% 
  mutate(y = as_factor(y), y = fct_collapse(y, `10+`=c("10","11","12","13","14","15","20"))) %>%
  group_by(y) %>% 
  reframe(observed = n()) %>%
  full_join(y = poisNBExp, by = join_by(y)) %>%
  kbl(booktabs = TRUE, escape = FALSE, align = "l", caption = "The distribution of cases with $1,2,\\cdots,10+$ cases of \\gls{LIST}.", col.names = c("Number of LIST\ncases","Number of times\nobserved", "Poisson\nexpected", "Negative binomial\nexpected"), digits = 2, linesep = "") %>%
  kable_paper(latex_options = c("HOLD_position"), full_width = TRUE)

```

Take, for example, the monthly cases of LIST analyzed in this thesis. Table \@ref(tab:PoisGexampleTbl) presents the distribution of these cases from 2008 to 2022, along with the expected numbers from both the Poisson and Negative binomial distributions. The distribution of cases over time is visualized in Figure \@ref(fig:PoisGexampleFig).

As observed, the actual distribution has notably heavier tails compared to the Poisson distribution. Additionally, the mean of the Poisson distribution for the monthly \gls{LIST} cases is $\hat{\lambda}=\bar{y}=`r meanYExample`$, while the empirical variance is $s^2=`r varYExample`$, which is considerably larger than the mean. These findings indicate overdispersion, suggesting that a Negative binomial model would be more appropriate for modeling the data.

(ref:PoisGexampleFig) Distribution of monthly cases of \gls{LIST} together with the expected numbers from both the Poisson and Negative binomial distributions

```{r PoisGexampleFig, echo=FALSE, out.width="80%", fig.cap="(ref:PoisGexampleFig)", fig.pos = "H", fig.show = "hold", fig.align='center'}

PoisGcustomPalette <- c("#990000","#2F3EEA","#1FD082")

LIST %>% 
  mutate(y = as_factor(y), y = fct_collapse(y, `10+`=c("10","11","12","13","14","15","20"))) %>%
  group_by(y) %>% 
  reframe(observed = n()) %>%
  full_join(y = poisNBExp, by = join_by(y)) %>%
  pivot_longer(cols = observed:`sum(NBExp)`) %>%
  mutate(name = factor(name,
                       levels = c("sum(poisExp)","observed", "sum(NBExp)"),
                       labels = c("Poisson exp.", "Observed", "Neg. Bin. exp.") )) %>%
  ggplot() +
  geom_linerange(mapping = aes(x = y, ymin = 0, ymax = value, group = name, colour = name), position = position_dodge2(width=0.3)) +
  geom_point(mapping = aes(x = y, y = value, group = name, shape = name, colour = name), position = position_dodge2(width=0.3)) +
  scale_shape_manual(name = "", values = c(1, 19, 2)) +
  scale_y_continuous(name = "Number of times observed") +
  scale_x_discrete(name = "Number of LIST cases") +
  scale_colour_manual(values = PoisGcustomPalette) +
  theme_bw() +
  guides(colour = "none", shape = guide_legend(override.aes = list(colour = PoisGcustomPalette, size = 2))) +
  theme(legend.position = "top",
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 15),
        legend.text = element_text(size = 12),
        legend.key.size = unit(1, 'cm'))

```
:::

\subsection{Formulation of the hierarchical model}

The hierarchical model used in this thesis to model the count observations $\boldsymbol y$ and assess the random effects $\boldsymbol u$ is presented in Definition \@ref(def:poisgTheorem), along with the derivation of the marginal distribution of $Y$ and the joint likelihood function.

::: {.definition #poisgTheorem name="Compound Poisson Gamma model"}
In the compound Poisson Gamma model the conditional distribution of the count observations are assumed to be a Poisson distribution with intensities $\lambda$

\begin{equation}\label{eq:pdfPois}
  f_{Y|u}(y;u,\boldsymbol{\beta})=\frac{(\lambda u)^{y}}{y!}\exp(-\lambda u)
\end{equation}

The probability density function for the random effects $\boldsymbol u$ are assumed to follow a reparametrized Gamma distribution with mean $1$, $u \sim \G(1/\phi,\phi)$ that is

\begin{equation} \label{eq:pdfGamma}
  f_{u}(u;\phi)=\frac{1}{\phi \Gamma(1/\phi)} \bigg(\frac{u}{\phi}\bigg)^{1/\phi-1} \exp (-u/\phi)
\end{equation}

Subsequently, the model can be formulated as a two-level hierarchical model

\begin{subequations} \label{eq:PoisGam}
  \begin{alignat}{2}
    \boldsymbol{Y|u} &\sim \Pois (\boldsymbol{\lambda u}) \label{eq:pois_g0} \\ 
    \boldsymbol{u} &\sim \G(\boldsymbol 1/\phi,\phi) \label{eq:pois_g1}
  \end{alignat}
\end{subequations}

Given \eqref{eq:pdfPois} and \eqref{eq:pdfGamma}, the probability function for the marginal distribution of $\boldsymbol Y$ is determined from

\begin{equation} \label{eq:marMix}
  \begin{aligned}
    g_{Y}(y;\beta,\phi)&=\int_{u=0}^\infty f_{Y|u}(y;u,\beta) f_{u}(u;\phi) \,du \\
    &=\int_{u=0}^\infty \frac{(\lambda u)^y}{y!} \exp (-\lambda u) \frac{1}{\phi \Gamma(1/\phi)} \bigg(\frac{u}{\phi}\bigg)^{1/\phi-1} \exp (-u /\phi) \,du\\
    &=\frac{\lambda^{y}}{y!\Gamma(1/\phi)\phi^{1/\phi}} \int_{u=0}^\infty u^{y+1/\phi-1} \exp \big(-u(\lambda \phi+1)/\phi\big) \,du
  \end{aligned}
\end{equation}

In \eqref{eq:marMix} it is noted that the integrand is the kernel in the probability density function for a Gamma distribution, $\G\big(y+1/\phi,\phi/(\lambda \phi+1)\big)$. As the integral of the density shall equal one, it is found by adjusting the norming constant that

\begin{equation}
  \int_{u=0}^\infty  u^{ y+ 1/\phi-1} \exp \Big(- u/\big(\phi/( \lambda \phi+1)\big)\Big) \,du = \frac{\phi^{ y+ 1/\phi}\Gamma( y+\boldsymbol 1/\phi)}{( \lambda \phi + 1)^{y+1/\phi}}
\end{equation}

Therefore, it is shown that the marginal distribution of $Y$ is a negative binomial distribution, $Y\sim\NB\big(1/\phi,1/(\lambda\phi+1)\big)$. The probability function for $Y$ is 

\begin{equation} \label{eq:pdfMix}
  \begin{aligned}
    P[Y=y]&=g_{Y}(y;\boldsymbol \beta, \phi) \\
    &=\frac{\lambda^{y}}{y!\Gamma(1/\phi)\phi^{1/\phi}}\frac{\phi^{y+1/\phi}\Gamma(y+1/\phi)}{(\lambda \phi + 1)^{y+1/\phi}} \\
    &=\frac{\Gamma(y+1/\phi)}{\Gamma(1/\phi)y!}\frac{1}{(\lambda\phi+1)^{1/\phi}}\bigg(\frac{\lambda\phi}{\lambda\phi+1}\bigg)^{y} \\
    &=\begin{pmatrix} y+1/\phi-1 \\ y \end{pmatrix} \frac{1}{(\lambda\phi+1)^{1/\phi}}\bigg(\frac{\lambda\phi}{\lambda\phi+1}\bigg)^{y} \ , \quad \mathrm{for} \ y = 0, 1, 2, \dots
  \end{aligned}
\end{equation}

where the following convention is used

\begin{equation}
  \begin{pmatrix} z\\y \end{pmatrix} = \frac{\Gamma(z+1)}{\Gamma(z+1-y)y!}
\end{equation}

for $z$ real and $y$ integer values. Consequently, the mean and variance of $Y$ are given by

\begin{equation}\label{eq:meanNB}
  \E[Y] = \lambda \qquad \V[Y] = \lambda (\lambda \phi + 1)
\end{equation}

The joint likelihood function for estimating $(\boldsymbol \beta,\phi)$ is

\begin{equation}\label{eq:jnllPoisG}
  L(\boldsymbol \beta, \phi; y_{it})=\prod_{t=1}^{T}\prod_{i=1}^{m} \begin{pmatrix} y_{it}+1/\phi-1 \\ y_{it} \end{pmatrix} \frac{1}{(\lambda_{it}\phi+1)^{1/\phi}}\bigg(\frac{\lambda_{it}\phi}{\lambda_{it}\phi+1}\bigg)^{y_{it}}
\end{equation}
:::

\subsection{Inference on individual groups}

Consider the compound Poisson Gamma model in \eqref{eq:PoisGam}, and assume that a value $Y=y$ has been observed.

The conditional distribution of $u$ for given $Y=y$ is found using Bayes Theorem. In order to simplify the notation, the subscript indicating the group and time are omitted. 

\begin{equation}
  \begin{aligned}
    g_{u}(u|Y=y)&=\frac{f_{y,u}(y,u)}{g_Y(y;\lambda, \phi)} \\
    &=\frac{f_{y|u}(y;u)g_{u}(u)}{g_{Y}(y;\lambda,\phi)} \\
    &=\frac{1}{g_{Y}(y;\lambda,\phi)}\Bigg(\frac{(\lambda u)^y}{y!} \exp (-\lambda u) \frac{1}{\phi \Gamma(1/\phi)} \bigg(\frac{u}{\phi}\bigg)^{1/\phi-1} \exp (-u/\phi)\Bigg) \\
    &\propto u^{y+1/\phi-1} \exp \big(- u(\lambda\phi+1)/\phi\big)
  \end{aligned}
\end{equation}

Here, the \textit{kernel} of the probability density function is identified

\begin{equation}
  u^{y+1/\phi-1} \exp (- u(\lambda\phi+1)/\phi)
\end{equation}

as the kernel of a Gamma distribution, $\G(y+1/\phi,\phi/(\lambda\phi+1))$, i.e. the conditional distribution of $u$ for given $Y=y$ can be written as

\begin{equation}
  u| Y=y\sim \G\big(y+1/\phi,\phi/(\lambda \phi+1)\big)
\end{equation}

The mean of the conditional distribution is given by:

\begin{equation}
  \E[u|Y=y]=\frac{y\phi+1}{\lambda \phi+1}
\end{equation}

And the variance of the conditional distribution is:                                            
\begin{equation}
  \V[u|Y=y]=\frac{( \phi^2+\phi)}{(\lambda \phi + 1)^2}
\end{equation}

These formulas provide the mean and variance of the conditional distribution of $u$ given the observed value $Y=y$. 

\subsection{Why is the Gamma distribution chosen to represent the variation between months?}

The choice of the Gamma distribution for modeling the random effects has been motivated by several reasons. Firstly, the support of the Gamma distribution, which ranges from 0 to infinity, aligns with the mean-value space, denoted as $\mathcal{M}$, for the Poisson distribution. This ensures that the random effects are constrained within a meaningful range for the underlying Poisson process.

Secondly, the two-parameter family of Gamma distributions offers considerable flexibility, encompassing a wide range of shapes and distributions that can span from exponential-like distributions to fairly symmetrical distributions on the positive real line. This flexibility allows the model to capture various patterns and characteristics observed in the data.

Additionally, the choice of the Gamma distribution has benefits in terms of the derivation of the marginal distribution of the response variable $Y$. The kernel $u^{\alpha-1}\exp(-u/\beta)$ of the Gamma distribution used for modeling the random effects exhibits a similar structure to the kernel $u^y\exp(-u)$ of the likelihood function corresponding to the sampling distribution of $Y$. This similarity facilitates the analytical computation of the integral involved in deriving the marginal distribution, as it can be expressed in terms of known functions.

Overall, the Gamma distribution is selected due to its alignment with the mean-value space of the Poisson distribution, its flexibility in capturing diverse distributions, and its analytical convenience in computing the marginal distribution of the response variable.

\section{Parameter estimation}

In this section, the parameter estimation and implementation of the models used in the novel outbreak detection algorithm are presented. These model are implemented in R using the open-source R package \textbf{TMB} (Template Model Builder) developed by @Kristensen_2016. This package facilitates efficient maximum likelihood estimation and uncertainty calculations for the parameter set $\boldsymbol \theta=(\boldsymbol{\beta, \Psi})$ and random effects $\boldsymbol u$. The presentation of the parameter estimation conducted in \textbf{TMB} is strongly inspired by Chapter 2 in @Kristensen_2016 and Section 5.10 in @Madsen_2010.

\textbf{TMB} maximizes a user-provided objective function in the form of a C++ template, to estimate the maximum likelihood for the parameter set $\boldsymbol \theta = (\boldsymbol{\beta, \Psi})$. In the following code chunk, the C++ template for the hierarchical Poisson normal model specified in \eqref{eq:PoisN}, is shown:

```{Rcpp, eval=FALSE}
#include <TMB.hpp>
template<class Type>
Type objective_function<Type>::operator() ()
{
  // R input data
  DATA_VECTOR(y);                               // Count data
  DATA_VECTOR(x);                               // Population size
  DATA_MATRIX(X);                               // Design matrix
  PARAMETER_VECTOR(u);                          // Random effects
  // Parameters
  PARAMETER_VECTOR(beta);                       // Fixed effects parameters
  PARAMETER(log_sigma_u);                       // Model parameter
  vector<Type> lambda  = exp(X*beta-log(x)+u);  // Construct 'lambda'
  Type sigma_u = exp(log_sigma_u);              // And the model parameters
  Type mean_ran = Type(0);
  // Objective function
  Type f = 0;                                   // Declare the objective
  f -= sum(dnorm(u,mean_ran,sigma_u,true));     // Calculate the objective
  f -= sum(dpois(y,lambda,true));               // Calculate the objective
  return f;
}
```



Refer to the associated GitHub repository at \href{https://github.com/telkamp7/AEDDO/blob/main/src/models/PoissonGamma.cpp}{src/models/PoissonGamma.cpp} to access the C++ template for the hierarchical Poisson Gamma model. The objective function maximizes the marginal log-likelihood function, which integrates out the random effects $\boldsymbol u$

\begin{equation}
  \ell_{M}(\boldsymbol{\theta; y})=\int_{\mathbb{R}^{q}} \ell (\boldsymbol{\theta;u,y}) d\boldsymbol{u}
\end{equation}

where $\ell(\boldsymbol{\theta, u,y})$ is the joint log-likelihood function of the data given the parameters and random effects. The maximizer $\hat{\boldsymbol u}_{\boldsymbol \theta}$ of the joint log-likelihood $\ell(\boldsymbol{\theta;u,y})$ with respect to the random effects $\boldsymbol u$ is defined as:

\begin{equation}
  \hat{\boldsymbol u}_{\boldsymbol \theta}=\argmax_{\boldsymbol u} \ell(\boldsymbol{\theta;u,y})
\end{equation}

Using $H(\hat{\boldsymbol u}_{\boldsymbol \theta})$ to denote the negative Hessian of the joint log-likelihood evaluated at $\hat{\boldsymbol u}_{\boldsymbol \theta}$; i.e,

\begin{equation}
  H(\hat{\boldsymbol u}_{\boldsymbol \theta}) =-\ell_{uu}''(\boldsymbol{\theta, u, y})|_{\boldsymbol u=\hat{\boldsymbol u}_{\boldsymbol \theta}}
\end{equation}

The Laplace approximation for the marginal log-likelihood $\ell_M(\boldsymbol \theta)$ is

\begin{equation}
  \ell_{M,LA}(\boldsymbol{\theta, y})=\ell(\boldsymbol{\theta,u,y})-\frac{1}{2}\log \Big|\frac{H(\hat{\boldsymbol u}_{\boldsymbol \theta})}{2\pi}\Big|
\end{equation}

Our estimate of $\theta$ minimizes the negative log of the Laplace approximation, i.e.,

\begin{equation}
  \hat{\boldsymbol\theta}=\argmin_{\boldsymbol\theta} - \ell_{M, LA}(\boldsymbol\theta, \boldsymbol y)
\end{equation}

The minimization of the Laplace approximation for the marginal likelihood is then performed using conventional R optimization routines (e.g., BFGS) to optimize the objective and obtain our estimate $\hat{\boldsymbol \theta}$. Uncertainty of the estimate $\hat{\boldsymbol \theta}$, or any differentiable function of the estimate $\phi(\hat{\boldsymbol \theta})$, is obtained by the $\delta$-method:

\begin{equation}
  \V\big(\phi(\hat{\boldsymbol \theta})\big)=-\phi_{\boldsymbol \theta}'(\hat{\boldsymbol \theta})\Big(\Delta^2 \ell_{M,LA}(\boldsymbol{\hat{\theta}, y})\Big)^{-1} \phi_{\boldsymbol \theta}'(\hat{\boldsymbol \theta})^T
\end{equation}

Additionally, \textbf{TMB} utilizes Automatic Differentiation (AD) techniques [@Griewank_2008] to evaluate first, second, and potentially third-order derivatives. This approach enhances the computational efficiency and accuracy of the parameter estimation process in the implemented models. Therefore, even though the random effects are analytically integrated out in the hierarchical Poisson Gamma model, and the Laplace approximation is not needed, implementing the joint log-likelihood function in \textbf{TMB} can still result in more efficient computations. For a comprehensive introduction to the concept of AD, it is recommended to read Section 2.1 and Section 2.2 of @Fournier_2012. 

It is important to note that the implementation of the hierarchical Poisson Normal model requires the use of the Laplace approximation to approximate the likelihood function. To investigate the impact on run time, the optimization of both the hierarchical Poisson Normal and Poisson Gamma models is compared using a simulated data set with $n=1000$. The simulated series $Y=\{y_1,\cdots,y_n\}$ follows a Poisson distribution with a rate of $\lambda=5$, i.e., $Y\sim\Pois(5)$. After generating the series, the following model for the fixed effects is optimized in both modeling frameworks:

\begin{equation}
  \log(\lambda) = \beta_{\text{Intercept}}
\end{equation}

Finally, the optimization routines for each of the modeling frameworks is run 100 times and the run time across these iterations are collected using the R package \textbf{microbenchmark}, which can be accessed from the \gls{CRAN} at \href{https://CRAN.R-project.org/package=microbenchmark}{ \nolinkurl{https://CRAN.R-project.org/package=microbenchmark}}. The resulting run times are visualized as a violin plot in Figure \@ref(fig:ComparePerformance).

(ref:ComparePerformance) A violin plot [@Jerry_1998] of the 100 run times for the optimization routines in each modeling framework.

```{r ComparePerformance, echo=FALSE, out.width="100%", fig.cap="(ref:ComparePerformance)", fig.pos = "H", fig.show = "hold"}
knitr::include_graphics("../figures/Compare_Performance.png")
```

Figure \@ref(fig:ComparePerformance) shows a violin plot comparing the running times for the optimization routines in each modeling framework. The plot demonstrates that the run time in the hierarchical Poisson Gamma model is remarkably faster, up to a factor of $4\times$, compared to the hierarchical Poisson Normal model. This indicates that the Poisson Gamma model may offer computational advantages in terms of run time efficiency.

\section{Scoring rule}

In this section, the scoring rule used to evaluate the overall score of the models is outlined. The approach is inspired by @Blicher_2021.  For a given time series ${y_t}={y_1,\dots,y_N}$, each forecast and its corresponding realized observation pair $(G_t,y_t)$ is evaluated. The overall score of the model is then reported as the average score:

\begin{equation}\label{eq:averageLogS}
\bar{S}(G,y)=\frac{1}{N} \sum_{t=1}^{N}S(G_t,y_t)
\end{equation}

One commonly used scoring rule is the \textit{logarithmic score} derived from likelihood theory, which is defined as $S(G,y)=-\log\big(G(y)\big)$ [@Good_1992; @Tilmann_2007]. In this thesis, this scoring rule is based on the probability mass function and is equivalent tot he log-likelihood of the forecast model. The calculation of the logarithmic score is shown in Example \@ref(exm:logs), which is adapted from @Blicher_2021.

::: {.example #logs name="Calculation of the logarithmic score"}

The Gamma distribution is used to represent the probabilistic forecast in this example. The Gamma distribution is parametrized by two parameters, shape ($\alpha$) and rate ($\beta$), and its probability density function (PDF) is given by:

\begin{equation}
f(y) = \frac{1}{\Gamma(\alpha)\beta}\left(\frac{y}{\beta}\right)^{\alpha-1}\exp\left(-\frac{y}{\beta}\right)
\end{equation}

In this example, the parameters of the true model, denoted as $f$, is chosen to be $(\alpha,\beta)=(3,3)$. 10 observations are simulated, denoted as $y_1,y_2,\dots,y_{10}$, which are shown in Table \@ref(tab:LogSExample). The true model $f$ is compared to a competing model, denoted as $g$, which is a Gamma distribution with parameters $(\alpha,\beta)=(3,8)$. The true model $f$, the competing model $g$, and the observations $\boldsymbol y$ are illustrated in Figure \@ref(fig:LogSExamplePlot).

```{r LogSExample, echo=FALSE}

set.seed(666)

y <- round(rgamma(n = 10, shape = 3, rate = 3),3)

t(tibble(`$i$` = paste(1:10), `$y_i$` = y)) %>% 
  kbl(booktabs = TRUE, escape = FALSE, caption = "10 simulated observations following a G(3,3)-distribution.") %>%
  kable_paper(full_width = TRUE, latex_options = "HOLD_position")

```

(ref:LogSExamplePlot) Observations (green dots) were simulated from a $\G(3,3)$-distribution. The true model $f$ (blue) is shown, along with the PDF for the competing model $g$, which follows a $G(3,8)$-distribution.

```{r LogSExamplePlot, echo=FALSE, out.width="80%", fig.cap="(ref:LogSExamplePlot)", fig.pos = "H", fig.show = "hold", fig.align='center'}

cust_palette <- c("#2F3EEA", "#990000")

plotData <- tibble(x = seq(from=0,to=3,length.out=10), y = y)

ggplot(data = plotData, aes(x = x)) +
  stat_function(fun = dgamma, args=list(3,3), linewidth = 1.2, aes(colour = "f(y) (True)")) +
  stat_function(fun = dgamma, args=list(3,8), linewidth = 1.2, aes(colour = "g(y) (Wrong)")) +
  geom_point(mapping = aes(x = y, y = 0), inherit.aes = FALSE, colour = "#008835", size = 2) +
  geom_point(mapping = aes(x = y, y = 0), inherit.aes = FALSE, pch = 1, size = 2) +
  scale_y_continuous(name = "PDF") +
  scale_x_continuous(name = "y") +
  scale_colour_manual(name = "", values = cust_palette) +
  theme_bw() +
  theme(legend.position = "top",
        axis.text = element_text(size = 12),
          axis.title = element_text(size = 15),
          legend.text = element_text(size = 12))

LogSf <- -dgamma(x = y, shape = 3, rate = 4, log = TRUE)
LogSg <- -dgamma(x = y, shape = 3, rate = 8, log = TRUE)

```

The logarithmic score of the true model $f$ for the first observation, $y_1=`r y[1]`$, is calculated as follows

\begin{equation}
  \begin{aligned}
  -\log\big(f(y_1)\big)&=-\log\big(f(`r y[1]`)\big) \\
  &=-\log\left[ \frac{1}{\Gamma(3)4}\Big(\frac{`r y[1]`}{4}\Big)^{3-1}\exp(-`r y[1]`/4)\right] \\
  &=`r round(dgamma(x = y[1], shape = 3, rate = 4, log = TRUE),3)`
  \end{aligned}
\end{equation}

Similarly, the logarithmic scores for the other observations can be calculated using the same formula. The individual logarithmic scores for all 10 observations are presented in Table \@ref(tab:tabLogS). Among the 10 observations, `r sum(LogSf < LogSg)` of them are more likely to occur under the true model $f$ compared to the competing model $g$. The final key quantity, the average logarithmic score, $\bar{S}(G,\boldsymbol y)$, can be calculated using Equation \eqref{eq:averageLogS}.

\begin{equation}
  \begin{aligned}
    \bar{S}(f,\boldsymbol y) &=`r round(mean(LogSf),2)` \\
    \bar{S}(g,\boldsymbol y) &=`r round(mean(LogSg),2)`
  \end{aligned}
\end{equation}

```{r tabLogS, echo=FALSE}

t(tibble(`$i$` = paste(1:10), `$S(f,y_i)$` = round(LogSf,2), `$S(g,y_i)$` = round(LogSg,2))) %>%
  kbl(booktabs = TRUE, escape = FALSE, caption = "Logarithmic scores of the two different Gamma models w.r.t. the 10 individual observations.") %>%
  kable_paper(full_width = TRUE, latex_options = "HOLD_position")

```
:::

The \textit{logarithmic score} has desirable properties as it captures all possible information about the observed data in relation to the model. However, it has a potential drawback in that it heavily penalizes unlikely observations. Consequently, even small changes in the tails of a density forecast can lead to significant changes in the \textit{logarithmic score}, even when the overall shape of the mass remains unchanged.

\cleardoublepage